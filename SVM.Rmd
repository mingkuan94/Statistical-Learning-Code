---
title: "SVM"
author: "Mingkuan"
date: '2019-10-13'
output: html_document
---

To demonstrate the SVM, it is easiest to work in low dimensions, so we can see the data.

Linear SVM classifier
----------------------
Let's get some data in two dimensions, and make them a little separated.
```{r}
set.seed(10111)
x = matrix(rnorm(40),20,2)
y = rep(c(-1,1),c(10,10))
x[y==1,]=x[y==1,]+1
plot(x,col=y+3,pch=19)
```
Now we will load the package `e1071` which contains the `svm` function we will use. We then compute the fit. Have to specify `cost` parameter, a tuning parameter.
```{r}
library(e1071)
dat = data.frame(x,y=as.factor(y))
svmfit = svm(y~., data=dat, kernel='linear',cost=10,scale=FALSE)
print(svmfit)
plot(svmfit,dat)
```
Make our own plot. The first thing we will do is make a grid of values for X1 and X2.

The support points (points on the margin, or on the wrong side of the margin) are indexed in the `$index` component of the fit.

```{r}
make.grid = function(x,n=75){
  grange=apply(x,2,range)
  x1 = seq(from=grange[1,1],to=grange[2,1],length=n)
  x2 = seq(from=grange[1,2],to=grange[2,2],length=n)
  expand.grid(X1=x1,X2=x2)
}
xgrid = make.grid(x) #dim=(75*75=5625,2)
ygrid = predict(svmfit,xgrid)
plot(xgrid,col=c('red','blue')[as.numeric(ygrid)],pch=20,cex=0.2)
points(x,col=y+3,pch=19)
points(x[svmfit$index,],pch=5,cex=2)
```

we extract the linear coefficients, and then using simple algebra, we include the decision boundary and the two margins.
```{r}
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0=svmfit$rho
plot(xgrid,col=c('red','blue')[as.numeric(ygrid)],pch=20,cex=0.2)
points(x,col=y+3,pch=19)
points(x[svmfit$index,],pch=5,cex=2)
abline(beta0/beta[2],-beta[1]/beta[2])
abline((beta0-1)/beta[2],-beta[1]/beta[2],lty=2)
abline((beta0+1)/beta[2],-beta[1]/beta[2],lty=2)
```

Nonlinear SVM
--------------
Load the mixture data from ESL where a non-linear boundary is called for.
```{r}
load(url("http://www.stanford.edu/~hastie/ElemStatLearn/datasets/ESL.mixture.rda"))
names(ESL.mixture)
rm(x,y)
attach(ESL.mixture)
```
2 dimensional data. Lets plot and fit a nonlinear SVM, using a radial kernel.
```{r}
plot(x,col=y+1)
dat = data.frame(y=factor(y),x)
fit = svm(factor(y)~., data=dat,scale=FALSE,kernel='radial',cost=5)
```
Now we create a grid and make prediction on the grid.
```{r}
xgrid = expand.grid(X1=px1,X2=px2)
ygrid = predict(fit,xgrid)
plot(xgrid,col=as.numeric(ygrid),pch=20,cex=0.2)
points(x,col=y+1,pch=19)
```

Include the actual estimates at each of our grid points using the contour function. `prob` on the data frame. If we plot its 0.5 contour, that will give us the _Bayes Decision Boundary_, which is the best one could ever do.

```{r}
func = predict(fit,xgrid,decision.values=TRUE)
func = attributes(func)$decision
xgrid = expand.grid(X1=px1,X2=px2)
ygrid = predict(fit,xgrid)
plot(xgrid,col=as.numeric(ygrid),pch=20,cex=0.2)
points(x,col=y+1,pch=19)

contour(px1,px2,matrix(func,69,99),level=0,add=TRUE)
contour(px1,px2,matrix(func,69,99),level=0.5,add=TRUE,col='blue',lwd=2)
```
```
